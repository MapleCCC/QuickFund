#!/usr/bin/env python3

import atexit
import locale
import os
import re
import shelve
import shutil
import threading
import traceback
from datetime import date, datetime, time, timedelta
from functools import lru_cache, partial
from itertools import filterfalse
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import attr
import click
import colorama
import xlsxwriter
from colorama import Fore
from tqdm import tqdm, trange
from tqdm.contrib import tenumerate, tmap
from tqdm.contrib.concurrent import thread_map
# GUI feature of tqdm is experimental. And our application is too fast for the plot to render.
# from tqdm.gui import tqdm, trange

from .__version__ import __version__
from .config import REPO_NAME, REPO_OWNER
from .fetcher import fetch_estimate, fetch_net_value
from .github_utils import get_latest_release_version
from .lru import LRU
from .schema import FundInfo
from .utils import (
    Logger,
    colored_console_context,
    parse_version_number,
    print_traceback_digest,
)

if locale.getdefaultlocale()[0] == "zh_CN":
    PERSISTENT_CACHE_DB_DIRECTORY = ".ç¼“å­˜"
else:
    PERSISTENT_CACHE_DB_DIRECTORY = ".cache"
# Instead of using full filename, we use basename, because shelve requires so.
PERSISTENT_CACHE_DB_FILE_BASENAME = "cache"
PERSISTENT_CACHE_DB_RECORD_MAX_NUM = 2000

ERR_LOG_FILE = "é”™è¯¯æ—¥å¿—.txt"

logger = Logger()


# Refer to https://github.com/tqdm/tqdm/issues/454
# FIXME: wait for the fix in upstream repository to land
if os.name == "nt":
    tqdm = partial(tqdm, ascii=True)
    trange = partial(trange, ascii=True)
    tmap = partial(tmap, ascii=True)
    tenumerate = partial(tenumerate, ascii=True)
    thread_map = partial(thread_map, ascii=True)


def write_to_xlsx(fund_infos: List[FundInfo], xlsx_filename: str) -> None:
    try:
        # TODO profile to see whether and how much setting constant_memory improves
        # performance.
        with xlsxwriter.Workbook(xlsx_filename, {"constant_memory": True}) as workbook:

            logger.log("æ–°å»º Excel æ–‡æ¡£......")
            worksheet = workbook.add_worksheet()

            # Widen column
            for i, field in enumerate(attr.fields(FundInfo)):
                width = field.metadata.get("width")
                # FIXME Despite the xlsxwriter doc saying that set_column(i, i, None) doesn't
                # change the column width, some simple tests show that it does. The source
                # code of xlsxwriter is too complex that I can't figure out how the
                # bug happens.
                worksheet.set_column(i, i, width)

            # Write header
            logger.log("å†™å…¥æ–‡æ¡£å¤´......")
            for i, field in enumerate(attr.fields(FundInfo)):
                header_format = workbook.add_format(
                    {"bold": True, "align": "center", "valign": "top", "border": 1}
                )
                worksheet.write_string(0, i, field.name, header_format)

            # Write body
            logger.log("å†™å…¥æ–‡æ¡£ä½“......")
            with colored_console_context(Fore.GREEN):  # type: ignore
                for row, info in tenumerate(
                    fund_infos, start=1, unit="è¡Œ", desc="å†™å…¥åŸºé‡‘ä¿¡æ¯"
                ):
                    for col, field in enumerate(attr.fields(FundInfo)):
                        # Judging from source code of xlsxwriter, add_format(None) is
                        # equivalent to default format.
                        cell_format = workbook.add_format(field.metadata.get("format"))
                        worksheet.write(row, col, info[col], cell_format)

            logger.log("Flush åˆ°ç¡¬ç›˜......")

    except Exception as exc:
        raise RuntimeError(f"è·å–åŸºé‡‘ä¿¡æ¯å¹¶å†™å…¥ Excel æ–‡æ¡£çš„æ—¶å€™å‘ç”Ÿé”™è¯¯") from exc


def check_args(in_filenames: Iterable[str], out_filename: str) -> None:
    for f in in_filenames:
        if not os.path.exists(f):
            raise FileNotFoundError(f"æ–‡ä»¶ {f} ä¸å­˜åœ¨")

    if os.path.isdir(out_filename):
        raise RuntimeError(f'åŒåæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œæ— æ³•æ–°å»ºæ–‡ä»¶ "{out_filename}"')

    if os.path.isfile(out_filename):

        if locale.getdefaultlocale()[0] == "zh_CN":
            backup_filename = "[å¤‡ä»½] " + out_filename
        else:
            backup_filename = out_filename + ".bak"

        try:
            shutil.move(out_filename, backup_filename)
        except PermissionError:
            raise RuntimeError(
                f"å¤‡ä»½ Excel æ–‡æ¡£æ—¶å‘ç”Ÿæƒé™é”™è¯¯ï¼Œæœ‰å¯èƒ½æ˜¯ Excel æ–‡æ¡£å·²ç»è¢«å…¶ä»–ç¨‹åºå ç”¨ï¼Œ"
                f'æœ‰å¯èƒ½æ˜¯ "{out_filename}" å·²ç»è¢« Excel æ‰“å¼€ï¼Œ'
                "è¯·å…³é—­æ–‡ä»¶ä¹‹åé‡è¯•"
            )
        logger.log(f'"{out_filename}" åŒåæ–‡ä»¶å·²å­˜åœ¨ï¼Œå¤‡ä»½è‡³ "{backup_filename}"')


def check_update() -> None:
    logger.log("è·å–æœ€æ–°åˆ†å‘ç‰ˆæœ¬å·......")
    # TODO Handle the case when the lastest release's tag name is not semantic
    # version.
    try:
        latest_version = get_latest_release_version(REPO_OWNER, REPO_NAME)
    except:
        logger.log("è·å–æœ€æ–°åˆ†å‘ç‰ˆæœ¬å·çš„æ—¶å€™å‘ç”Ÿé”™è¯¯ï¼Œæš‚æ—¶è·³è¿‡ã€‚å¯ä»¥é€šè¿‡ --update å‘½ä»¤æ¥æ‰‹åŠ¨è§¦å‘æ›´æ–°æ£€æŸ¥")
        return

    if parse_version_number(latest_version) > parse_version_number(__version__):
        logger.log(f"æ£€æµ‹åˆ°æ›´æ–°ç‰ˆæœ¬ {latest_version}ï¼Œè¯·æ‰‹åŠ¨æ›´æ–°")
        exit()
    else:
        logger.log("å½“å‰å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")


def net_value_date_is_latest(net_value_date: date) -> bool:
    # Take advantage of the knowledge that fund info stays the same
    # within 0:00 to 20:00.

    # WARNING: it's true that most of time the market is not opened
    # in weekends. But we can't use this knowledge in our logic. Because
    # sometimes holiday policy will make this irregular. We had better
    # fall back to use the most robust way to check.

    now_time = datetime.now().time()
    today = date.today()
    yesterday = today - timedelta(days=1)

    if time.min <= now_time < time(20):
        return net_value_date == yesterday
    else:
        return net_value_date == today


def estimate_datetime_is_latest(estimate_datetime: datetime) -> bool:
    # Take advantage of the knowledge that estimate info stays the same
    # within 15:00 to next day 15:00.

    # WARNING: it's true that most of time the market is not opened
    # in weekends. But we can't use this knowledge in our logic. Because
    # sometimes holiday policy will make this irregular. We had better
    # fall back to use the most robust way to check.

    open_market_time = time(9, 30)
    close_market_time = time(15)
    now_time = datetime.now().time()
    today = date.today()
    yesterday = today - timedelta(days=1)
    today_close_market_datetime = datetime.combine(today, close_market_time)
    yesterday_close_market_datetime = datetime.combine(yesterday, close_market_time)

    if open_market_time <= now_time <= close_market_time:
        return False
    elif time.min <= now_time < open_market_time:
        return estimate_datetime == yesterday_close_market_datetime
    elif close_market_time < now_time <= time.max:
        return estimate_datetime == today_close_market_datetime
    else:
        raise RuntimeError("Unreachable")


def get_fund_infos(fund_codes: List[str]) -> List[FundInfo]:
    if not os.path.isdir(PERSISTENT_CACHE_DB_DIRECTORY):
        os.makedirs(PERSISTENT_CACHE_DB_DIRECTORY)

    shelf_path = os.path.join(
        PERSISTENT_CACHE_DB_DIRECTORY, PERSISTENT_CACHE_DB_FILE_BASENAME
    )

    with shelve.open(shelf_path) as fund_info_cache_db:
        renewed_variable_access_lock = threading.Lock()
        renewed: Dict[str, FundInfo] = {}

        @lru_cache(maxsize=None)
        def get_fund_info(fund_code: str) -> FundInfo:
            need_renew = False
            fund_info: FundInfo = fund_info_cache_db.get(fund_code, FundInfo())

            net_value_date = fund_info.å‡€å€¼æ—¥æœŸ
            if net_value_date is None or not net_value_date_is_latest(net_value_date):
                need_renew = True
                data = fetch_net_value(fund_code)
                fund_info.åŸºé‡‘ä»£ç  = data.åŸºé‡‘ä»£ç 
                fund_info.å‡€å€¼æ—¥æœŸ = data.å‡€å€¼æ—¥æœŸ
                fund_info.å•ä½å‡€å€¼ = data.å•ä½å‡€å€¼
                fund_info.æ—¥å¢é•¿ç‡ = data.æ—¥å¢é•¿ç‡
                fund_info.åˆ†çº¢é€é… = data.åˆ†çº¢é€é…
                fund_info.ä¸Šä¸€å¤©å‡€å€¼ = data.ä¸Šä¸€å¤©å‡€å€¼
                fund_info.ä¸Šä¸€å¤©å‡€å€¼æ—¥æœŸ = data.ä¸Šä¸€å¤©å‡€å€¼æ—¥æœŸ

            estimate_datetime = fund_info.ä¼°ç®—æ—¥æœŸ
            if estimate_datetime is None or not estimate_datetime_is_latest(
                estimate_datetime
            ):
                need_renew = True
                data = fetch_estimate(fund_code)
                fund_info.åŸºé‡‘ä»£ç  = data.åŸºé‡‘ä»£ç 
                fund_info.åŸºé‡‘åç§° = data.åŸºé‡‘åç§°
                fund_info.ä¼°ç®—æ—¥æœŸ = data.ä¼°ç®—æ—¥æœŸ
                fund_info.å®æ—¶ä¼°å€¼ = data.å®æ—¶ä¼°å€¼
                fund_info.ä¼°ç®—å¢é•¿ç‡ = data.ä¼°ç®—å¢é•¿ç‡

            if need_renew:
                # TIPS: Uncomment following line to profile lock congestion.
                # print(renewed_variable_access_lock.locked())
                renewed_variable_access_lock.acquire()
                renewed[fund_code] = fund_info
                renewed_variable_access_lock.release()

            return fund_info

        # FIXME experiment to find a suitable number as threshold between sync and
        # async code
        if len(fund_codes) < 3:
            with colored_console_context(Fore.GREEN):  # type: ignore
                fund_infos = list(
                    tmap(get_fund_info, fund_codes, unit="ä¸ª", desc="è·å–åŸºé‡‘ä¿¡æ¯")
                )
        else:
            with colored_console_context(Fore.GREEN):  # type: ignore
                fund_infos = thread_map(
                    get_fund_info, fund_codes, unit="ä¸ª", desc="è·å–åŸºé‡‘ä¿¡æ¯"
                )

        logger.log("å°†åŸºé‡‘ç›¸å…³ä¿¡æ¯å†™å…¥æ•°æ®åº“ï¼Œç•™å¤‡ä¸‹æ¬¡ä½¿ç”¨ï¼ŒåŠ é€Ÿä¸‹æ¬¡æŸ¥è¯¢......")
        fund_info_cache_db.update(renewed)

        logger.log("æ›´æ–°ç¼“å­˜ LRU ä¿¡æ¯......")

        # TODO remove out-dated cache entries

        # Instead of directly in-place updating the "lru_record" entry in
        # fund_info_cache_db, we copy it to a new variable and update the
        # new variable and then copy back. This is because directly in-place
        # updating shelve dict entry requires opening shelve with the `writeback`
        # parameter set to True, which could lead to increased memory cost
        # and IO cost and slow down the program.

        lru = fund_info_cache_db.get("lru_record", LRU())
        lru.batch_update(fund_codes)

        if len(lru) > PERSISTENT_CACHE_DB_RECORD_MAX_NUM:
            logger.log("æ£€æµ‹åˆ°ç¼“å­˜è¾ƒå¤§ï¼Œæ¸…ç†ç¼“å­˜......")
            to_evict_num = PERSISTENT_CACHE_DB_RECORD_MAX_NUM - len(lru)
            for _ in trange(to_evict_num, unit="æ¡", desc="æ¸…ç†ç¼“å­˜"):
                del fund_info_cache_db[lru.evict()]

        fund_info_cache_db["lru_record"] = lru

        return fund_infos


def validate_fund_code(s: str) -> bool:
    return bool(re.fullmatch(r"[0-9]{6}", s))


@click.command()
@click.argument("files_or_fund_codes", nargs=-1)
@click.option("-o", "--output", default="åŸºé‡‘ä¿¡æ¯.xlsx")
@click.option("--disable-update-check", is_flag=True, default=False)
# TODO: @click.option("--update")
@click.version_option(version=__version__)
def main(
    files_or_fund_codes: Tuple[str], output: str, disable_update_check: bool
) -> None:
    try:
        # atexit.register(lambda _: input("Press ENTER to exit"))
        atexit.register(lambda: input("æŒ‰ä¸‹å›è½¦é”®ä»¥é€€å‡º"))

        colorama.init()

        # TODO Remove update check logic after switching architecture to
        # server/client model
        if not disable_update_check:
            print("æ£€æŸ¥æ›´æ–°......")
            check_update()

        in_filenames = filterfalse(validate_fund_code, files_or_fund_codes)
        out_filename = output

        logger.log("æ£€æŸ¥å‚æ•°......")
        check_args(in_filenames, out_filename)

        logger.log("è·å–åŸºé‡‘ä»£ç åˆ—è¡¨......")
        fund_codes = []
        for x in files_or_fund_codes:
            if validate_fund_code(x):
                fund_codes.append(x)
            else:
                lines = Path(x).read_text(encoding="utf-8").splitlines()
                cleaned_lines = map(str.strip, lines)
                fund_codes.extend(filter(validate_fund_code, cleaned_lines))

        if not fund_codes:
            logger.log("æ²¡æœ‰å‘ç°åŸºé‡‘ä»£ç ")
            exit()

        logger.log("è·å–åŸºé‡‘ç›¸å…³ä¿¡æ¯......")
        fund_infos = get_fund_infos(fund_codes)

        logger.log("å°†åŸºé‡‘ç›¸å…³ä¿¡æ¯å†™å…¥ Excel æ–‡ä»¶......")
        write_to_xlsx(fund_infos, out_filename)

        # The emoji takes inspiration from the black (https://github.com/psf/black)
        logger.log("å®Œæ»¡ç»“æŸ! âœ¨ ğŸ° âœ¨")
    except:
        logger.log("Oops! ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­é‡åˆ°äº†é”™è¯¯ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯æ‘˜è¦å¦‚ä¸‹ï¼š")
        print_traceback_digest()
        with open(ERR_LOG_FILE, "w", encoding="utf-8") as f:
            traceback.print_exc(file=f)
        logger.log(f'è¯¦ç»†é”™è¯¯ä¿¡æ¯å·²å†™å…¥æ—¥å¿—æ–‡ä»¶ "{ERR_LOG_FILE}"ï¼Œè¯·å°†æ—¥å¿—æ–‡ä»¶æäº¤ç»™å¼€å‘è€…è¿›è¡Œè°ƒè¯• debug')


if __name__ == "__main__":
    main()  # pylint: disable=no-value-for-parameter
